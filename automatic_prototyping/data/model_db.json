[{"id":"tuner007\/pegasus_summarizer","task":"summarization","description":"\n\n## Model description\n[PEGASUS](https:\/\/github.com\/google-research\/pegasus) fine-tuned for summarization\n\n## Install \"sentencepiece\" library required for tokenizer\n```\npip install sentencepiece\n```\n\n## Model in Action \ud83d\ude80\n```\nimport torch\nfrom transformers import PegasusForConditionalGeneration, PegasusTokenizer\nmodel_name = 'tuner007\/pegasus_summarizer'\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n\ndef get_response(input_text):\n  batch = tokenizer([input_text],truncation=True,padding='longest',max_length=1024, return_tensors=\"pt\").to(torch_device)\n  gen_out = model.generate(**batch,max_length=128,num_beams=5, num_return_sequences=1, temperature=1.5)\n  output_text = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n  return output_text\n```\n#### Example: \ncontext = \"\"\"\"\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli \"upset\". \"I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset,\" said Pant in a virtual press conference after the close of the first day\\'s play.\"You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess,\" he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120\/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52*) and Hameed (60*) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, \"They took the heavy roller, the wicket was much more settled down, and they batted nicely also,\" he said. \"But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.\"Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\n\"\"\"\n\n```\nget_response(context)\n```\n#### Output:\nTeam India wicketkeeper-batsman Rishabh Pant has said that Virat Kohli was \"upset\" after someone threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England. \"You can say whatever you want to chant, but don't throw things at the fielders and all those things. It's not good for cricket, I guess,\" Pant added.'\n\n#### [Inshort](https:\/\/www.inshorts.com\/) (60 words News summary app, rated 4.4 by 5,27,246+ users on android playstore) summary:\nIndia wicketkeeper-batsman Rishabh Pant has revealed that captain Virat Kohli was upset with the crowd during the first day of Leeds Test against England because someone threw a ball at pacer Mohammed Siraj. Pant added, \"You can say whatever you want to chant, but don't throw things at the fielders and all those things. It is not good for cricket.\"\n\n\n> Created by [Arpit Rajauria](https:\/\/twitter.com\/arpit_rajauria)\n[![Twitter icon](https:\/\/cdn0.iconfinder.com\/data\/icons\/shift-logotypes\/32\/Twitter-32.png)](https:\/\/twitter.com\/arpit_rajauria)\n","input_desc":"The input to this AI model for summarization is a chunk of text in the English language. Specifically, the example given is a news article about a cricket match between India and England, but any English language text could be used as input. In the example code provided, the input text is passed as a string variable called `context` to the `get_response()` function.","output_desc":"The output of this AI model is a summary of a given article or text. Specifically, it generates a condensed version of the input text that captures the most important information and key points. The length of the output summary can be set using the `max_length` parameter, and the number of possible summaries generated can be adjusted using the `num_return_sequences` parameter. The output is in the form of a string, which can be used for various applications and downstream tasks that require summarization, such as news aggregation and knowledge extraction."},{"id":"facebook\/wav2vec2-large-960h-lv60-self","task":"automatic-speech-recognition","description":"\n\n# Wav2Vec2-Large-960h-Lv60 + Self-Training\n\n[Facebook's Wav2Vec2](https:\/\/ai.facebook.com\/blog\/wav2vec-20-learning-the-structure-of-speech-from-raw-audio\/)\n\nThe large model pretrained and fine-tuned on 960 hours of Libri-Light and Librispeech on 16kHz sampled speech audio. Model was trained with [Self-Training objective](https:\/\/arxiv.org\/abs\/2010.11430). When using the model make sure that your speech input is also sampled at 16Khz.\n\n[Paper](https:\/\/arxiv.org\/abs\/2006.11477)\n\nAuthors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli\n\n**Abstract**\n\nWe show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8\/3.3 WER on the clean\/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8\/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.\n\nThe original model can be found under https:\/\/github.com\/pytorch\/fairseq\/tree\/master\/examples\/wav2vec#wav2vec-20.\n\n\n# Usage\n\nTo transcribe audio files the model can be used as a standalone acoustic model as follows:\n\n```python\n from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n from datasets import load_dataset\n import torch\n \n # load model and processor\n processor = Wav2Vec2Processor.from_pretrained(\"facebook\/wav2vec2-large-960h-lv60-self\")\n model = Wav2Vec2ForCTC.from_pretrained(\"facebook\/wav2vec2-large-960h-lv60-self\")\n     \n # load dummy dataset and read soundfiles\n ds = load_dataset(\"patrickvonplaten\/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n \n # tokenize\n input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values\n \n # retrieve logits\n logits = model(input_values).logits\n \n # take argmax and decode\n predicted_ids = torch.argmax(logits, dim=-1)\n transcription = processor.batch_decode(predicted_ids)\n ```\n \n  ## Evaluation\n \n This code snippet shows how to evaluate **facebook\/wav2vec2-large-960h-lv60-self** on LibriSpeech's \"clean\" and \"other\" test data.\n \n```python\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport torch\nfrom jiwer import wer\n\n\nlibrispeech_eval = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook\/wav2vec2-large-960h-lv60-self\").to(\"cuda\")\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook\/wav2vec2-large-960h-lv60-self\")\n\ndef map_to_pred(batch):\n    inputs = processor(batch[\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\")\n    input_values = inputs.input_values.to(\"cuda\")\n    attention_mask = inputs.attention_mask.to(\"cuda\")\n    \n    with torch.no_grad():\n        logits = model(input_values, attention_mask=attention_mask).logits\n\n    predicted_ids = torch.argmax(logits, dim=-1)\n    transcription = processor.batch_decode(predicted_ids)\n    batch[\"transcription\"] = transcription\n    return batch\n\nresult = librispeech_eval.map(map_to_pred, remove_columns=[\"audio\"])\n\nprint(\"WER:\", wer(result[\"text\"], result[\"transcription\"]))\n```\n\n*Result (WER)*:\n\n| \"clean\" | \"other\" |\n|","input_desc":"The inputs for this AI model are speech audio files sampled at 16kHz. The model expects the audio input to be in a format that can be read and processed by the Wav2Vec2 processor. In the provided code snippets, the input audio files are loaded from a dataset and tokenized using the Wav2Vec2 processor before being passed into the model for transcribing.","output_desc":"The Wav2Vec2-Large-960h-Lv60 + Self-Training AI model outputs transcriptions of speech audio files, using a standalone acoustic model. It returns the predicted transcription and the confidence or probability scores representing the model's level of certainty in its transcription. The model is evaluated using the Word Error Rate (WER) metric on LibriSpeech's \"clean\" and \"other\" test data."},{"id":"cafeai\/cafe_aesthetic","task":"image-classification","description":"\n\n# Info\n\nSince people are downloading this and I don't know why, I'll add some information. This model is an image classifier fine-tuned on `microsoft\/beit-base-patch16-384`.\nIts purpose is to be used in the dataset conditioning step for the [Waifu Diffusion project](https:\/\/huggingface.co\/hakurei\/waifu-diffusion), a fine-tune effort for Stable Diffusion. As WD1.4 is planned to have a *significantly large dataset* (~15m images), it is infeasible to analyze every image manually to determine whether or not it should be included in the final training dataset. This image classifier is trained on approximately 3.5k real-life and anime\/manga images. Its purpose is to remove aesthetically worthless images from our dataset by classifying them as \"`not_aesthetic`\". The image classifier was trained to **err on the side of caution** and will generally tend to include images unless they are in a \"manga-like\" format, have messy lines and\/or are sketches, or include an unacceptable amount of text (namely text that covers the primary subject of the image). The idea is that certain images will hurt a SD fine-tune.\n\nNote: This classifier is not perfect, just like every other classifier out there. However, with a sufficiently large dataset, any imperfections or misclassifications should average themselves out due to the Law of Large Numbers.\n\nYou can test out the classifier [here](https:\/\/huggingface.co\/spaces\/cafeai\/cafe_aesthetic_demo), along with some other classifiers for the project.\n\n\n# License\nReleased under the aGPLv3. Use the model as you wish for any purpose. If you make changes, share the changes.","input_desc":"The inputs for this AI model are images. The model is intended to be an image classifier, which means it takes an input image and assigns it to a particular category or label based on its features and characteristics. In this case, the model has been specifically trained to distinguish between aesthetically valuable and aesthetically worthless images, so the input images are likely to be a mix of real-life and anime\/manga images, with varying levels of visual style, complexity, and content. The model is","output_desc":"The outputs of this AI model are binary classifications of whether an image is aesthetically valuable or not. Specifically, the model classifies images as \"not_aesthetic\" if it determines them to be of poor quality, such as having messy lines or too much text covering the primary subject. On the other hand, the model will generally include images unless they fall under the aforementioned categories. The classifier ultimately aims to remove images that may hurt a Stable Diffusion fine-tune, but it is not perfect and may misclassify some images."},{"id":"nateraw\/food","task":"image-classification","description":"\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# nateraw\/food\n\nThis model is a fine-tuned version of [google\/vit-base-patch16-224-in21k](https:\/\/huggingface.co\/google\/vit-base-patch16-224-in21k) on the nateraw\/food101 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4501\n- Accuracy: 0.8913\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0002\n- train_batch_size: 128\n- eval_batch_size: 128\n- seed: 1337\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy |\n|:","input_desc":"---|---|---|---|---|\n| 1.023 | 1 | 125 | 0.522 | 0.824 |\n| 0.568 | 2 | 250 | 0.486 | 0.849 |\n| 0.464 | 3 | 375 | 0.468 | 0.857 |\n| 0.401 | 4 | 500 | 0.450 | 0.865 |\n| ","output_desc":"---:|:---:|:---:|:---:|:---:|\n| 1.3881 | 1.0 | 723\/723 | 0.8563 | 0.7258 |\n| 0.5998 | 2.0 | 723\/723 | 0.6721 | 0.7862 |\n| 0.3645 | 3.0 | 723\/723 | 0.5631 | 0.8256 |\n| 0.2287 | 4.0 | 723\/723 | 0.4890 | 0.8498 |\n| 0.1405 | 5.0 | 723\/723 | 0.4501 | 0.8913 |\n\nThe model outputs food image classifications. Specifically, it takes an image of food as input and outputs a predicted food category, such as \"pasta\", \"steak\", or \""},{"id":"facebook\/textless_sm_it_fr","task":"audio-to-audio","description":"\nYou can try out the model on the right of the page by uploading or recording.\nFor model usage, please refer to https:\/\/huggingface.co\/facebook\/textless_sm_cs_en\n","input_desc":"Based on the provided information, it seems that the AI model is a language model designed by Facebook and available through Hugging Face platform. The model is specialized in Czech (cs) and English (en) languages, and it seems that it doesn't require any text input from the user. Instead, the model can be tested or applied on a provided dataset or text corpus. The description only encourages users to upload or record data for testing, but it doesn't specify what kind of data or in what","output_desc":"Unfortunately, the provided description does not give any information about the outputs of the AI model. The link provided seems to be a resource for the model usage rather than its outputs. Without further context about the model's purpose or function, it is difficult to provide a clear description of its outputs."},{"id":"m3hrdadfi\/wav2vec2-large-xlsr-persian-v3","task":"automatic-speech-recognition","description":"\n\n# Wav2Vec2-Large-XLSR-53-Persian V3\n\n\n## Usage\nFine-tuned [facebook\/wav2vec2-large-xlsr-53](https:\/\/huggingface.co\/facebook\/wav2vec2-large-xlsr-53) in Persian (Farsi) using [Common Voice](https:\/\/huggingface.co\/datasets\/common_voice). When using this model, make sure that your speech input is sampled at 16kHz.\n\n\n**Requirements**\n```bash\n# requirement packages\n!pip install git+https:\/\/github.com\/huggingface\/datasets.git\n!pip install git+https:\/\/github.com\/huggingface\/transformers.git\n!pip install torchaudio\n!pip install librosa\n!pip install jiwer\n!pip install parsivar\n!pip install num2fawords\n```\n\n**Normalizer**\n```bash\n# Normalizer\n!wget -O normalizer.py https:\/\/huggingface.co\/m3hrdadfi\/\"wav2vec2-large-xlsr-persian-v3\/raw\/main\/dictionary.py\n!wget -O normalizer.py https:\/\/huggingface.co\/m3hrdadfi\/\"wav2vec2-large-xlsr-persian-v3\/raw\/main\/normalizer.py\n```\n\n**Downloading data**\n```bash\nwget https:\/\/voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com\/cv-corpus-6.1-2020-12-11\/fa.tar.gz\n\ntar -xzf fa.tar.gz\nrm -rf fa.tar.gz\n```\n\n**Cleaning**\n```python\nfrom normalizer import normalizer\n\ndef cleaning(text):\n    if not isinstance(text, str):\n        return None\n\n    return normalizer({\"sentence\": text}, return_dict=False)\n\ndata_dir = \"\/content\/cv-corpus-6.1-2020-12-11\/fa\"\n\ntest = pd.read_csv(f\"{data_dir}\/test.tsv\", sep=\"\t\")\ntest[\"path\"] = data_dir + \"\/clips\/\" + test[\"path\"]\nprint(f\"Step 0: {len(test)}\")\n\ntest[\"status\"] = test[\"path\"].apply(lambda path: True if os.path.exists(path) else None)\ntest = test.dropna(subset=[\"path\"])\ntest = test.drop(\"status\", 1)\nprint(f\"Step 1: {len(test)}\")\n\ntest[\"sentence\"] = test[\"sentence\"].apply(lambda t: cleaning(t))\ntest = test.dropna(subset=[\"sentence\"])\nprint(f\"Step 2: {len(test)}\")\n\ntest = test.reset_index(drop=True)\nprint(test.head())\n\ntest = test[[\"path\", \"sentence\"]]\ntest.to_csv(\"\/content\/test.csv\", sep=\"\t\", encoding=\"utf-8\", index=False)\n```\n\n**Prediction**\n```python\nimport numpy as np\nimport pandas as pd\n\nimport librosa\nimport torch\nimport torchaudio\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nfrom datasets import load_dataset, load_metric\n\nimport IPython.display as ipd\n\nmodel_name_or_path = \"m3hrdadfi\/wav2vec2-large-xlsr-persian-v3\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(model_name_or_path, device)\n\nprocessor = Wav2Vec2Processor.from_pretrained(model_name_or_path)\nmodel = Wav2Vec2ForCTC.from_pretrained(model_name_or_path).to(device)\n\n\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    speech_array = speech_array.squeeze().numpy()\n    speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, processor.feature_extractor.sampling_rate)\n\n    batch[\"speech\"] = speech_array\n    return batch\n\n\ndef predict(batch):\n    features = processor(\n        batch[\"speech\"], \n        sampling_rate=processor.feature_extractor.sampling_rate, \n        return_tensors=\"pt\", \n        padding=True\n    )\n\n    input_values = features.input_values.to(device)\n    attention_mask = features.attention_mask.to(device)\n\n    with torch.no_grad():\n        logits = model(input_values, attention_mask=attention_mask).logits \n\n    pred_ids = torch.argmax(logits, dim=-1)\n\n    batch[\"predicted\"] = processor.batch_decode(pred_ids)\n    return batch\n\n\ndataset = load_dataset(\"csv\", data_files={\"test\": \"\/content\/test.csv\"}, delimiter=\"\t\")[\"test\"]\ndataset = dataset.map(speech_file_to_array_fn)\nresult = dataset.map(predict, batched=True, batch_size=4)\n```\n\n**WER Score**\n```python\nwer = load_metric(\"wer\")\nprint(\"WER: {:.2f}\".format(100 * wer.compute(predictions=result[\"predicted\"], references=result[\"sentence\"])))\n```\n\n**Output**\n```python\nmax_items = np.random.randint(0, len(result), 20).tolist()\nfor i in max_items:\n    reference, predicted =  result[\"sentence\"][i], result[\"predicted\"][i]\n    print(\"reference:\", reference)\n    print(\"predicted:\", predicted)\n    print('","input_desc":"The input to this AI model is an audio speech signal sampled at a rate of 16kHz. The speech data needs to be provided in the form of a file path to the audio file. The model is fine-tuned in the Persian (Farsi) language using the Common Voice dataset, which consists of audio files and their corresponding transcriptions. The model also requires installation packages such as torchaudio, librosa, jiwer, parsivar, and num2fawords. Before making","output_desc":"The model, Wav2Vec2-Large-XLSR-53-Persian V3, takes speech input in Persian and returns predicted transcripts for the speech input. The output is a string representing the predicted transcription of the speech input in Persian. Additionally, the model provides a Word Error Rate (WER) score as an output, which represents the percentage of words that were incorrectly predicted in the transcription when compared to the actual speech input."},{"id":"facebook\/deit-base-distilled-patch16-224","task":"image-classification","description":"\n\n# Distilled Data-efficient Image Transformer (base-sized model)\n\nDistilled data-efficient Image Transformer (DeiT) model pre-trained and fine-tuned on ImageNet-1k (1 million images, 1,000 classes) at resolution 224x224. It was first introduced in the paper [Training data-efficient image transformers & distillation through attention](https:\/\/arxiv.org\/abs\/2012.12877) by Touvron et al. and first released in [this repository](https:\/\/github.com\/facebookresearch\/deit). However, the weights were converted from the [timm repository](https:\/\/github.com\/rwightman\/pytorch-image-models) by Ross Wightman.  \n\nDisclaimer: The team releasing DeiT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThis model is a distilled Vision Transformer (ViT). It uses a distillation token, besides the class token, to effectively learn from a teacher (CNN) during both pre-training and fine-tuning. The distillation token is learned through backpropagation, by interacting with the class ([CLS]) and patch tokens through the self-attention layers.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. \n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https:\/\/huggingface.co\/models?search=facebook\/deit) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nSince this model is a distilled ViT model, you can plug it into DeiTModel, DeiTForImageClassification or DeiTForImageClassificationWithTeacher. Note that the model expects the data to be prepared using DeiTFeatureExtractor. Here we use AutoFeatureExtractor, which will automatically use the appropriate feature extractor given the model name. \n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import AutoFeatureExtractor, DeiTForImageClassificationWithTeacher\nfrom PIL import Image\nimport requests\n\nurl = 'http:\/\/images.cocodataset.org\/val2017\/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained('facebook\/deit-base-distilled-patch16-224')\nmodel = DeiTForImageClassificationWithTeacher.from_pretrained('facebook\/deit-base-distilled-patch16-224')\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\n# forward pass\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\nCurrently, both the feature extractor and model support PyTorch. Tensorflow and JAX\/FLAX are coming soon.\n\n## Training data\n\nThis model was pretrained and fine-tuned with distillation on [ImageNet-1k](http:\/\/www.image-net.org\/challenges\/LSVRC\/2012\/), a dataset consisting of 1 million images and 1k classes. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training\/validation can be found [here](https:\/\/github.com\/facebookresearch\/deit\/blob\/ab5715372db8c6cad5740714b2216d55aeae052e\/datasets.py#L78). \n\nAt inference time, images are resized\/rescaled to the same resolution (256x256), center-cropped at 224x224 and normalized across the RGB channels with the ImageNet mean and standard deviation.\n\n### Pretraining\n\nThe model was trained on a single 8-GPU node for 3 days. Training resolution is 224. For all hyperparameters (such as batch size and learning rate) we refer to table 9 of the original paper.\n\n## Evaluation results\n\n| Model                                 | ImageNet top-1 accuracy | ImageNet top-5 accuracy | # params | URL                                                              |\n|","input_desc":"The inputs for this AI model are images which are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. The model expects the data to be prepared using DeiTFeatureExtractor.","output_desc":"The outputs of this AI model are the predicted class probabilities for a given input image. Specifically, the model predicts one of the 1,000 ImageNet classes, and the final layer of the model provides a vector of probabilities over these classes. These predicted probabilities can be used to identify the most likely class for the input image and perform subsequent actions based on that classification, such as labeling the image or triggering downstream processes."},{"id":"bigscience\/bloom","task":"text-generation","description":"\n\n<img src=\"https:\/\/s3.amazonaws.com\/moonup\/production\/uploads\/1657124309515-5f17f0a0925b9863e28ad517.png\" alt=\"BigScience Logo\" width=\"800\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"\/>\n\nBigScience Large Open-science Open-access Multilingual Language Model  \nVersion 1.3 \/ 6 July 2022\n\nCurrent Checkpoint: **Training Iteration  95000**\n\nLink to paper: [here](https:\/\/arxiv.org\/abs\/2211.05100)\n\nTotal seen tokens: **366B**\n\n","input_desc":"The description of the AI model does not provide information about the inputs it takes. It only provides information about the model name, version, and training iteration, as well as the total seen tokens and a link to the related paper.","output_desc":"The above description of an AI model does not provide any information on the outputs that it generates. It only gives details about the model and its version number, the current checkpoint, the link to the paper describing it, and the total number of tokens seen during training. Outputs depend on the specific implementation and intended application of the model, which are not described here."},{"id":"tufa15nik\/vilt-finetuned-vqasi","task":"visual-question-answering","description":"Entry not found","input_desc":"I'm sorry, can you please provide me with the description of the AI model so that I can give you a description of the inputs?","output_desc":"I'm sorry, could you please provide me with a description of the AI model you're referring to, so I can give you the description of its outputs?"},{"id":"keras-io\/deeplabv3p-resnet50","task":"image-segmentation","description":"\n\n## Multiclass semantic segmentation using DeepLabV3+\nThis repo contains the model and the notebook [to this Keras example on Multiclass semantic segmentation using DeepLabV3+](https:\/\/keras.io\/examples\/vision\/deeplabv3_plus\/).\n\nFull credits to: [Soumik Rakshit](http:\/\/github.com\/soumik12345)\n\nThe model is trained for demonstrative purposes and does not guarantee the best results in production. For better results, follow & optimize the [Keras example]((https:\/\/keras.io\/examples\/vision\/deeplabv3_plus\/) as per your need.\n\n## Background Information \nSemantic segmentation, with the goal to assign semantic labels to every pixel in an image, is an essential computer vision task. In this example, we implement the DeepLabV3+ model for multi-class semantic segmentation, a fully-convolutional architecture that performs well on semantic segmentation benchmarks.   \n\n## Training Data\nThe model is trained on a subset (10,000 images) of [Crowd Instance-level Human Parsing Dataset](https:\/\/arxiv.org\/abs\/1811.12596). The Crowd Instance-level Human Parsing (CIHP) dataset has 38,280 diverse human images. Each image in CIHP is labeled with pixel-wise annotations for 20 categories, as well as instance-level identification. This dataset can be used for the \"human part segmentation\" task.\n\n## Model\nThe model uses ResNet50 pretrained on ImageNet as the backbone model.\n\nReferences:   \n1. [Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation](https:\/\/arxiv.org\/pdf\/1802.02611.pdf)   \n2. [Rethinking Atrous Convolution for Semantic Image Segmentation](https:\/\/arxiv.org\/abs\/1706.05587)   \n3. [DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs](https:\/\/arxiv.org\/abs\/1606.00915)","input_desc":"The AI model described is used for multi-class semantic segmentation, which means it takes an image as input and assigns a semantic label to each pixel in the image. The input to the model would therefore be an image, likely in the form of a digital file (e.g. JPEG, PNG, etc.). The specific images used to train the model come from a subset of the Crowd Instance-level Human Parsing Dataset, which contains diverse human images with pixel-wise annotations for 20 categories. The model uses Res","output_desc":"The output of this AI model is pixel-wise semantic labels of a given image. It assigns one of the 20 predetermined semantic categories to every pixel in the image, providing a segmentation map of the image. This segmentation map can be used for identifying the human parts in an image or for performing other similar tasks. The model takes an image as input and produces a corresponding pixel-wise semantic label map as output."}]