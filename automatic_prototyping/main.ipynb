{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample model db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.openai.requests import input_description, output_description\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(path_or_buf=\"data/huggingface_models.jsonl\", lines=True)\n",
    "model_db = data[[\"id\",\"task\",\"description\"]].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_db[\"input_desc\"] = model_db.description.apply(input_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_db[\"output_desc\"] = model_db.description.apply(output_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to json\n",
    "model_db.to_json(\"data/model_db.json\", orient=\"records\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.task_planning.task_planning import zero_shot_task_planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_task_plan = zero_shot_task_planning(\"a model that takes as input text from user change it to a poem and classify it as positive emotion or negative emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = eval(zero_task_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'task_id': 1,\n",
       "  'task': 'text-classification',\n",
       "  'task_description': 'Classify given text as positive or negative emotion',\n",
       "  'dep': [],\n",
       "  'inputs': [{'id': 1,\n",
       "    'input_type': 'text',\n",
       "    'input_description': 'Text input to classify'}],\n",
       "  'outputs': [{'id': 2,\n",
       "    'output_type': 'text',\n",
       "    'output_description': \"Classified emotion ('positive' or 'negative')\"}]},\n",
       " {'task_id': 2,\n",
       "  'task': 'text2text-generation',\n",
       "  'task_description': 'Generate a poem from given text input',\n",
       "  'dep': [1],\n",
       "  'inputs': [{'id': 1,\n",
       "    'input_type': 'text',\n",
       "    'input_description': 'Text input to generate poem'}],\n",
       "  'outputs': [{'id': 3,\n",
       "    'output_type': 'text',\n",
       "    'output_description': 'Generated poem'}]},\n",
       " {'task_id': 3,\n",
       "  'task': 'text-classification',\n",
       "  'task_description': 'Classify generated poem as positive or negative emotion',\n",
       "  'dep': [2],\n",
       "  'inputs': [{'id': 3,\n",
       "    'input_type': 'text',\n",
       "    'input_description': 'Generated poem to classify'}],\n",
       "  'outputs': [{'id': 4,\n",
       "    'output_type': 'text',\n",
       "    'output_description': \"Classified emotion of generated poem ('positive' or 'negative')\"}]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"\n",
    "we need to make it easy for our employees to add and edit regulations. \n",
    "Adding a new regulation requires multiple checks across thousands of pages of already existing regulations. \n",
    "During these checks (which takes months) employees look for conflicting regulations made earlier, \n",
    "loop holes and blind spots.\n",
    "find the relevant regulations, paragraphs and articles to the request\n",
    "check if a conflict occurs\n",
    "explain the conflict occurring\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_task_plan= zero_shot_task_planning(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = eval(test_task_plan)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject:str, task:str, description:str, inputs:dict, outputs:dict\n",
    "def problem(**kwargs):\n",
    "    return str(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_1 = problem(subject=\"document embedding\", \n",
    "        task=\"text splitting\", \n",
    "        description=\"divide text into chunks\", \n",
    "        inputs=\"[document text : str]\", \n",
    "        outputs=\"[list of sentences : list[str]]\")\n",
    "\n",
    "problem_2 = problem(subject=\"document embedding\", \n",
    "        task=\"text splitting\", \n",
    "        description=\"divide text into chunks\", \n",
    "        inputs=\"[document text : str]\", \n",
    "        outputs=\"[list of sentences : list[str]]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = [\n",
    "\"\"\"\n",
    "{\"subject\" : document embedding\n",
    "task : text splitting\n",
    "description : divide text into chunks\n",
    "inputs : [document text : str]\n",
    "output : list of sentences : list[str]}\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "\"subject\" : document embedding\n",
    "task : pdf to text\n",
    "description : turn the pdf into text while ignoring images\n",
    "inputs : [input documentation pdf : pdf]\n",
    "output : document text : str\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "\"subject\" : document embedding\n",
    "task : sentence embedding\n",
    "description : embed each sentence\n",
    "inputs : [list of sentences : list[str]]\n",
    "output : list of list of embeddings and list of sentences : list[list[str], list[float]]] \n",
    "\"\"\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hachem/Documents/ai-sdk/.auto_prototype/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.model_selection.model_selection import SemanticSearchEngine\n",
    "from src.task_planning.test_task_plans import test_task_plan_1, test_task_plan_2, test_task_plan_3\n",
    "from src.utils.utils import read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_db = read_json(\"data/model_db.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks': [{'task_id': '1',\n",
       "   'task': 'find_relevant_regulations',\n",
       "   'task_description': 'Find relevant regulations, paragraphs and articles to the request',\n",
       "   'dep': [],\n",
       "   'inputs': [{'id': '1a',\n",
       "     'input_type': 'text',\n",
       "     'input_description': 'Regulation search query'}],\n",
       "   'outputs': [{'id': '2a',\n",
       "     'output_type': 'text',\n",
       "     'output_description': 'List of relevant regulations'}]},\n",
       "  {'task_id': '2',\n",
       "   'task': 'check_for_conflict',\n",
       "   'task_description': 'Check if a conflict occurs within the relevant regulations found in the previous task',\n",
       "   'dep': ['1'],\n",
       "   'inputs': [{'id': '2a',\n",
       "     'input_type': 'text',\n",
       "     'input_description': 'List of relevant regulations'}],\n",
       "   'outputs': [{'id': '3a',\n",
       "     'output_type': 'text',\n",
       "     'output_description': 'List of conflicting regulations'}]},\n",
       "  {'task_id': '3',\n",
       "   'task': 'explain_conflict',\n",
       "   'task_description': 'Explain the conflict occurring',\n",
       "   'dep': ['2'],\n",
       "   'inputs': [{'id': '3a',\n",
       "     'input_type': 'text',\n",
       "     'input_description': 'List of conflicting regulations'},\n",
       "    {'id': '1a',\n",
       "     'input_type': 'text',\n",
       "     'input_description': 'Regulation search query'}],\n",
       "   'outputs': [{'id': '4a',\n",
       "     'output_type': 'text',\n",
       "     'output_description': 'Explanation of the conflict occurring'}]}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_task_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['facebook/textless_sm_it_fr',\n",
       "  'cafeai/cafe_aesthetic',\n",
       "  'tuner007/pegasus_summarizer',\n",
       "  'keras-io/deeplabv3p-resnet50',\n",
       "  'nateraw/food'],\n",
       " ['tuner007/pegasus_summarizer',\n",
       "  'facebook/textless_sm_it_fr',\n",
       "  'm3hrdadfi/wav2vec2-large-xlsr-persian-v3',\n",
       "  'facebook/wav2vec2-large-960h-lv60-self',\n",
       "  'bigscience/bloom'],\n",
       " ['cafeai/cafe_aesthetic',\n",
       "  'tuner007/pegasus_summarizer',\n",
       "  'facebook/textless_sm_it_fr',\n",
       "  'm3hrdadfi/wav2vec2-large-xlsr-persian-v3',\n",
       "  'nateraw/food']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se = SemanticSearchEngine()\n",
    "se.forward(model_db=model_db, task_db=final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
